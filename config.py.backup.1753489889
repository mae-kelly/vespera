import os
import torch
import platform
import subprocess
import sys

MODE = os.getenv("MODE", "dry")
LIVE_MODE = MODE == "live"
ASSETS = ["BTC", "ETH", "SOL"]

SIGNAL_CONFIDENCE_THRESHOLD = 0.7
POSITION_SIZE_PERCENT = 2.0
MAX_OPEN_POSITIONS = 3
MAX_DRAWDOWN_PERCENT = 10.0
COOLDOWN_MINUTES = 5

OKX_API_LIMITS = {
    "orders_per_second": 20,
    "requests_per_second": 10,
    "max_position_size": 50000
}

DISCORD_WEBHOOK_URL = os.getenv("DISCORD_WEBHOOK_URL")
DISCORD_USER_ID = os.getenv("DISCORD_USER_ID")

def detect_mac_gpu_capabilities():
    """Exhaustive Mac GPU detection - M1/M2/M3 priority"""
    system = platform.system()
    if system != "Darwin":
        return None
    
    gpu_info = {
        "has_metal": False,
        "has_discrete_gpu": False,
        "gpu_names": [],
        "memory_gb": 0,
        "metal_family": None,
        "architecture": platform.machine(),
        "apple_silicon": False
    }
    
    # Priority 1: Detect Apple Silicon first
    try:
        result = subprocess.run(['sysctl', '-n', 'machdep.cpu.brand_string'], 
                              capture_output=True, text=True, timeout=5)
        if result.returncode == 0:
            cpu_info = result.stdout.strip()
            print(f"üîç CPU Detection: {cpu_info}")
            
            if 'Apple' in cpu_info and any(chip in cpu_info for chip in ['M1', 'M2', 'M3', 'M4']):
                gpu_info["apple_silicon"] = True
                gpu_info["has_metal"] = True
                gpu_info["metal_family"] = "Apple Silicon"
                
                # Extract chip type
                for chip in ['M1', 'M2', 'M3', 'M4']:
                    if chip in cpu_info:
                        gpu_info["gpu_names"].append(f"Apple {chip} GPU")
                        print(f"üöÄ APPLE SILICON DETECTED: {chip}")
                        break
    except:
        pass
    
    # Priority 2: System profiler GPU scan
    try:
        result = subprocess.run(['system_profiler', 'SPDisplaysDataType'], 
                              capture_output=True, text=True, timeout=10)
        if result.returncode == 0:
            output = result.stdout
            print(f"üîç Hardware Scan Results:")
            
            if 'Metal' in output:
                gpu_info["has_metal"] = True
                print("‚úÖ Metal support confirmed")
                
            # Look for GPU names
            lines = output.split('\n')
            for line in lines:
                if 'Chipset Model:' in line:
                    gpu_name = line.split(':')[-1].strip()
                    if gpu_name and gpu_name not in gpu_info["gpu_names"]:
                        gpu_info["gpu_names"].append(gpu_name)
                        print(f"‚úÖ GPU Found: {gpu_name}")
                        
                if any(keyword in line for keyword in ['Metal Family:', 'Metal GPUFamily']):
                    print(f"‚úÖ Metal Family: {line.strip()}")
                    
                if 'VRAM' in line or 'Graphics Memory' in line:
                    try:
                        memory_str = line.split(':')[-1].strip()
                        if 'GB' in memory_str:
                            memory_val = float(memory_str.replace('GB', '').strip())
                            gpu_info["memory_gb"] = max(gpu_info["memory_gb"], memory_val)
                    except:
                        pass
    except:
        pass
    
    return gpu_info

def setup_gpu_acceleration():
    """HIERARCHICAL GPU DETECTION: M1/M2/M3 ‚Üí CUDA ‚Üí CPU"""
    system = platform.system()
    machine = platform.machine()
    
    print(f"üîç HIERARCHICAL GPU DETECTION")
    print(f"System: {system} {machine}")
    print(f"PyTorch version: {torch.__version__}")
    
    # PRIORITY 1: A100 GPUs (HIGHEST PERFORMANCE)
    print("\nüöÄ PRIORITY 1: A100 GPU DETECTION")
    print("=" * 40)
    
    if torch.cuda.is_available():
        try:
            device_name = torch.cuda.get_device_name(0)
            print(f"‚úÖ CUDA GPU found: {device_name}")
            
            # Check if it's an A100 first (highest priority)
            if "A100" in device_name:
                print("üèÜ A100 DETECTED - MAXIMUM PERFORMANCE MODE")
                test_tensor = torch.randn(100, device='cuda')
                result = torch.sum(test_tensor)
                print(f"‚úÖ A100 GPU test: {result:.3f}")
                
                # Enable A100 optimizations
                torch.backends.cuda.matmul.allow_tf32 = True
                torch.backends.cudnn.allow_tf32 = True
                torch.backends.cudnn.benchmark = True
                torch.cuda.empty_cache()
                
                # Setup CuPy for A100
                try:
                    import cupy as cp
                    cp.cuda.Device(0).use()
                    mempool = cp.get_default_memory_pool()
                    mempool.set_limit(size=2**33)  # 8GB limit
                    print("‚úÖ A100 CuPy optimization enabled")
                except:
                    print("‚ÑπÔ∏è CuPy optimization skipped")
                
                return {
                    "type": "cuda_a100",
                    "device": "cuda",
                    "optimized": True,
                    "gpu_info": {"name": device_name, "a100": True},
                    "priority": 1
                }
        except Exception as e:
            print(f"‚ö†Ô∏è A100 test failed: {e}")
    else:
        print("‚ùå No CUDA GPUs available")
    
    # PRIORITY 2: APPLE SILICON (M1/M2/M3/M4) - STRONG SECOND CHOICE
    if system == "Darwin":
        print("\nüçé PRIORITY 2: APPLE SILICON DETECTION")
        print("=" * 40)
        
        gpu_info = detect_mac_gpu_capabilities()
        
        if gpu_info and gpu_info["apple_silicon"]:
            print(f"‚úÖ Apple Silicon confirmed: {gpu_info['gpu_names']}")
            
            # Test MPS (Metal Performance Shaders)
            if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                try:
                    print("üß™ Testing Apple Silicon GPU...")
                    test_tensor = torch.randn(100, device='mps')
                    result = torch.sum(test_tensor)
                    print(f"‚úÖ APPLE SILICON GPU ACTIVE: {result:.3f}")
                    
                    return {
                        "type": "apple_silicon",
                        "device": "mps",
                        "optimized": True,
                        "gpu_info": gpu_info,
                        "priority": 2
                    }
                except Exception as e:
                    print(f"‚ö†Ô∏è Apple Silicon GPU test failed: {e}")
                    # Try with fallback
                    try:
                        os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'
                        test_tensor = torch.randn(10, device='mps')
                        result = torch.sum(test_tensor)
                        print(f"‚úÖ Apple Silicon GPU with fallback: {result:.3f}")
                        return {
                            "type": "apple_silicon_fallback",
                            "device": "mps", 
                            "optimized": True, 
                            "gpu_info": gpu_info,
                            "priority": 2
                        }
                    except:
                        print("‚ùå Apple Silicon GPU fallback failed")
            else:
                print("‚ùå MPS not available for Apple Silicon")
        
        # Check for discrete GPUs on Mac
        if gpu_info and gpu_info["has_discrete_gpu"]:
            print("\nüîç Checking for discrete GPU on Mac...")
    
    # PRIORITY 3: OTHER CUDA GPUs (Non-A100)
    print("\n‚ö° PRIORITY 3: OTHER CUDA GPU DETECTION")
    print("=" * 40)
    
    if torch.cuda.is_available():
        try:
            device_name = torch.cuda.get_device_name(0)
            print(f"‚úÖ Other CUDA GPU found: {device_name}")
            
            # Test other CUDA GPUs (non-A100)
            test_tensor = torch.randn(10, device='cuda')
            result = torch.sum(test_tensor)
            print(f"‚úÖ CUDA GPU test: {result:.3f}")
            
            torch.backends.cudnn.benchmark = True
            return {
                "type": "cuda_standard",
                "device": "cuda",
                "optimized": True,
                "gpu_info": {"name": device_name},
                "priority": 3
            }
        except Exception as e:
            print(f"‚ùå Other CUDA test failed: {e}")
    else:
        print("‚ùå No other CUDA GPUs available")
    
    # PRIORITY 4: OTHER MAC GPUS (Intel, AMD)
    if system == "Darwin" and gpu_info and gpu_info["has_metal"]:
        print("\nüîç PRIORITY 4: OTHER MAC GPU DETECTION")
        print("=" * 40)
        
        if gpu_info["has_discrete_gpu"]:
            print("‚úÖ Discrete GPU detected on Mac")
            return {
                "type": "mac_discrete",
                "device": "cpu",  # May need CPU fallback for some discrete GPUs
                "optimized": False,
                "gpu_info": gpu_info,
                "priority": 4
            }
    
    # PRIORITY 5: CPU FALLBACK (ABSOLUTE LAST RESORT)
    print("\nüíª PRIORITY 5: CPU FALLBACK")
    print("=" * 40)
    print("‚ö†Ô∏è No GPU acceleration available - using CPU")
    print("üí° Consider upgrading PyTorch or checking GPU drivers")
    
    return {
        "type": "cpu_fallback",
        "device": "cpu",
        "optimized": False,
        "gpu_info": None,
        "priority": 5
    }

def validate_config():
    errors = []
    
    if SIGNAL_CONFIDENCE_THRESHOLD <= 0 or SIGNAL_CONFIDENCE_THRESHOLD > 1:
        errors.append("SIGNAL_CONFIDENCE_THRESHOLD must be between 0 and 1")
    
    if POSITION_SIZE_PERCENT <= 0 or POSITION_SIZE_PERCENT > 100:
        errors.append("POSITION_SIZE_PERCENT must be between 0 and 100")
    
    if MAX_OPEN_POSITIONS <= 0:
        errors.append("MAX_OPEN_POSITIONS must be positive")
    
    if not ASSETS or len(ASSETS) == 0:
        errors.append("ASSETS list cannot be empty")
    
    return errors

# Initialize GPU with proper hierarchy
GPU_CONFIG = setup_gpu_acceleration()
GPU_AVAILABLE = GPU_CONFIG["optimized"]
DEVICE = GPU_CONFIG["device"]
MAC_GPU_INFO = GPU_CONFIG.get("gpu_info")

# Validate configuration
config_errors = validate_config()
if config_errors:
    print("‚ùå CONFIGURATION ERRORS:")
    for error in config_errors:
        print(f"   - {error}")
    print("‚ùå Fix configuration before starting system!")
else:
    print(f"\n‚úÖ FINAL GPU CONFIG:")
    print(f"   Type: {GPU_CONFIG['type']}")
    print(f"   Device: {DEVICE}")
    print(f"   Optimized: {GPU_AVAILABLE}")
    print(f"   Priority: {GPU_CONFIG.get('priority', 'Unknown')}")
    
    if MAC_GPU_INFO and MAC_GPU_INFO.get("gpu_names"):
        print(f"   GPU: {', '.join(MAC_GPU_INFO['gpu_names'])}")
    
    print(f"   Assets: {ASSETS}")
    print(f"   Mode: {MODE}")
    
    # Show hierarchy used
    if GPU_CONFIG["priority"] == 1:
        print("üèÜ USING A100 GPU (Maximum Performance)")
    elif GPU_CONFIG["priority"] == 2:
        print("üçé USING APPLE SILICON GPU (Strong Second Choice)")
    elif GPU_CONFIG["priority"] == 3:
        print("‚ö° USING OTHER CUDA GPU (Third Priority)")
    elif GPU_CONFIG["priority"] == 4:
        print("üîç USING OTHER MAC GPU (Fourth Priority)")
    else:
        print("üíª USING CPU FALLBACK (Last Resort)")
